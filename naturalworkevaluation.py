import os.path
import random
import re
from turtle import pd
from typing import Optional, Dict

import matplotlib.pyplot as plt
import numpy
import pandas as pd
from pandas import np

from dominator_mutants import convert_csv_to_killmap, \
    calculate_dominating_mutants, convert_csv_to_unique_killmap, \
    convert_csv_to_unique_reverse_killmap


# results_dir is the directory where the results are stored is
def generate_all_natural_work_evaluation(results_dir):
    """ Generates the work evaluation for all natural mutants.

    It grabs the csv/mml files generated by major framework for natural
    mutants. Then, it creates a plot that illustrates the test completeness
    achieved for all natural mutants generated for a given bug.
    See documentation for convert_csv_to_killmap,
        convert_csv_to_reverse_killmap, generate_scores,
        combine_mapping, and generate_eval_plot for more details

    :param results_dir: str
                        The directory containing the results files from
                        tailored mutant data
    :return: tuple (list[int], int)
                    A tuple containing a list of y-cordinates (as a percentage)
                    for the work evaluation for all natural mutants. and the
                    number of mutants for the evaluation
    """

    # keeping the count of all plots and counts
    plots: Optional[list] = list()
    counts: Optional[list] = list()

    dirpath = results_dir + "natural-mutants\\non-triggering"

    # step 2 fetch the killmap
    killmap = convert_csv_to_unique_killmap(
        os.path.join(dirpath, "killMap.csv"))
    rev_killmap = convert_csv_to_unique_reverse_killmap(os.path.join(dirpath,
                                                                     "killMap.csv"))

    #  getting the total number of mutants for
    total_number_of_mutants = 0
    for mutant in killmap.keys():
        total_number_of_mutants += len(mutant)

    # step 3 in a dictionary (may have to change this) containing from the mutant identifier to token identifiers:
    #  {mutantID: token, sub-token} . the sub-token would be -1 if itâ€™s the default subtoken.

    log_file_path = os.path.join(dirpath, "mutants.log")
    mutant_token_sub_mapping: Optional[dict] = \
        (generate_mutant_to_token_mapping(log_file_path))

    # step 4.1
    mml_file_path = os.path.join(dirpath, "mml_confidence_data.csv")
    token_to_score_mapping: Optional[dict] = generate_scores(mml_file_path)

    # step 4.2
    mutant_to_scores_mapping = combine_mapping(mutant_token_sub_mapping,
                                               token_to_score_mapping)

    # Filter mutant_to_scores_mapping for mutants only in the killmap
    filtererd_mutant_to_scores_mapping: Optional[dict] = dict()
    for mutant in mutant_to_scores_mapping.keys():
        for killmap_mutants in killmap.keys():
            if mutant in killmap_mutants:
                filtererd_mutant_to_scores_mapping[killmap_mutants] = \
                    mutant_to_scores_mapping[mutant]

    # step 5
    # randomize mutants
    sorted_mutants = sorted(filtererd_mutant_to_scores_mapping.keys(),
                            key=lambda i:
                            -np.log(filtererd_mutant_to_scores_mapping[i][0] /
                                    filtererd_mutant_to_scores_mapping[i][1]))

    work_eval_plot = generate_eval_plot(sorted_mutants, killmap, rev_killmap,
                                        total_number_of_mutants)
    plots.append(work_eval_plot[0])
    counts.append(work_eval_plot[1])
    return plots, counts


# TODO document
def plot_generator(results_dir, type):
    dirpath = results_dir + "\\" + type + "\\non-triggering"
    killmap = convert_csv_to_killmap(os.path.join(dirpath, "killMap.csv"))
    result = generate_test_completeness_plot(killmap)
    return result


def mutants_average(results_dir, type, number_of_trials=10):
    """ Runs evaluate_traditional_mutants_randomly for given number of times
    and averages the results before returning it. See documenation for
    evaluate_traditional_mutants_randomly

    :param results_dir: str
                    The directory containing the results files from tailored
                    mutant data
    :param work_eval_plots_counts: tuple(list[int], count)
                    The output from all natural mutants
    evaluation for the same bug that is being evalutated in by this function
    :param number_of_trials: int
                the number of random trails that will conducted and then
                averaged for a given bug
    :return: tuple (list[int], int)
                    A tuple containing a list of y-cordinates (as a percentage)
                    for the work evaluation for random traditional mutants.
                    and the number of mutants for the evaluation
    """

    results_list: Optional[list] = list()
    results_list_shortened: Optional[list] = list()
    results_list_length_holder: Optional[list] = list()

    min_size = np.inf
    max_size = 0
    for i in range(0, number_of_trials):
        temp_result = baseline_generator(results_dir, type)[0]
        results_list.append(temp_result)
        if min_size > len(temp_result):
            min_size = len(temp_result)

    for i in range(0, number_of_trials):
        results_list_shortened.append(results_list[i][:min_size])

        if len(results_list[i]) > min_size:
            results_list_length_holder.append(
                (i, len(results_list[i]) - min_size))
            if max_size < len(results_list[i]) - min_size:
                max_size = len(results_list[i]) - min_size

    # print("results list length holder",results_list_length_holder)
    results_list_additional_sums = [0] * max_size
    results_list_length_count = [0] * max_size
    # print(max_size, len(results_list_additional_sums))

    for row, random_result_length in results_list_length_holder:
        for col in range(min_size, min_size + random_result_length):
            # print(row, col, col - min_size, random_result_length)
            results_list_length_count[col - min_size] = 1 + \
                                                        results_list_length_count[
                                                            col - min_size]
            results_list_additional_sums[col - min_size] += \
                results_list[row][col]

    # print(results_list_length_count)
    # print("sums", results_list_additional_sums)
    result = np.mean([i for i in results_list_shortened], axis=0)
    # print(result)

    # adding the extra part at the end
    for i in range(len(results_list_length_count)):
        result = numpy.append(result, results_list_additional_sums[
            i] / results_list_length_count[i])

    # print(result)

    return result


# TODO fix documentation
def baseline_generator(results_dir, type):
    """ Generates the work evaluation for random traditional mutants.

        It grabs the csv/mml files generated by major framework for traditional
        mutants. Then, it creates a plot that illustrates the test completeness
        achieved for random traditional mutants generated for a given bug.
        See documentation for convert_csv_to_killmap,
        convert_csv_to_reverse_killmap, generate_scores,
        combine_mapping, and generate_eval_plot for more details

        :param results_dir: str
                            The directory containing the results files from
                            tailored mutant data
        :return: tuple (list[int], int)
                        A tuple containing a list of y-cordinates (as a percentage)
                        for the work evaluation for random traditional mutants
                        and the number of mutants for the evaluation
    """

    dirpath = results_dir + "\\" + type + "\\non-triggering"
    # step 2 fetch the killmap
    killmap = convert_csv_to_unique_killmap(
        os.path.join(dirpath, "killMap.csv"))
    rev_killmap = convert_csv_to_unique_reverse_killmap(os.path.join(dirpath,
                                                                     "killMap.csv"))

    #  getting the total number of mutants for
    total_number_of_mutants = 0
    for mutant in killmap.keys():
        total_number_of_mutants += len(mutant)

    killmap_mutants = killmap.keys()
    list_of_mutant: Optional[list] = list()
    for mutant in killmap_mutants:
        list_of_mutant.append(mutant)

    randomized_mutants = random.sample(list_of_mutant, len(killmap))

    random_eval_plot = generate_eval_plot(
        randomized_mutants, killmap, rev_killmap,
        total_number_of_mutants)
    return random_eval_plot


def generate_mutant_to_token_mapping(log_file):
    """ Given mutants.log file from tailored mutants data for a bug,
    it creates a mapping from the mutant identifier to its token identifier


    :param log_file: str
                The path to mutants.log generated by tailored mutant data
    :return: Dict[int, int[]
            A dictionary containing a mapping from the mutant identifier to
            its token identifier
    """
    mutant_to_token_mapping: Optional[Dict[int, int[-1, -1]]] = dict()
    tokenization_pattern = re.compile(
        r"([0-9]*);[A-Z]*;([0-9]*);((([0-9]*[.]*)[ ]*[a-z]*[A-Z]*[.]*[:]*[-]*[,]*[_]*[\"]*[ ]*[']*[()]*[\[]*[\]]*)*)")
    with open(log_file, 'r') as fo:
        for line in fo:
            header_match = tokenization_pattern.match(line)
            if header_match:
                if header_match.group(3).startswith("\'") or header_match.group(
                        3).startswith("\""):
                    subtoken = header_match.group(3)[1:-1]
                else:
                    subtoken = header_match.group(3)
                mutant_to_token_mapping[int(header_match.group(1))] = [
                    int(header_match.group(2)), subtoken]
            continue
            raise ValueError("No pattern matched line: {}".format(line))

        return mutant_to_token_mapping


def generate_scores(mml_csv):
    """ Uses mml_confidence_data.csv file from tailored mutants
    data for a given bug. It creates a mapping from the token identifiers for
    that bug to their naturalness scores.

    :param log_file: str
                The path to mutants.log generated by tailored mutant data
    :return: Dict[int, int[]
            A dictionary containing a mapping from the mutant identifier to
            its token identifier
    """
    token_prob_position_score: Optional[Dict[int[-1, -1], int[-1, -1]]] = dict()
    score_reading_pattern = re.compile(
        r"[A-Z_(]*([\d]*)[)]<([\d]*[\"]*[']*[ ]*[a-z]*[A-Z]*[.]*[:]*["
        r"\_]*[(]*[)]*[>]*)*-> ([\"'\d*[a-z]]*[.a-zA-Z0-9_\"',<> $?\[\]:("
        r")\\-]*)#(\d.\d*[A-Z]*-*[0-9]*)*#(\d.\d*[A-Z]*-*[0-9]*)*#[a-z]*#("
        r"\d.\d*)([A-Z]-[0-9])*#[a-z]*#[a-z]*#(\d.\d*)([A-Z]-[0-9])*")

    with open(mml_csv, 'r') as fo:
        for line in fo:
            if line.strip() == "\n":
                continue
            header2_match = score_reading_pattern.match(line)
            # group 1 -> token number identifier
            # group 3 -> subtoken number identifier
            # group 5 -> first
            # group 9 -> second
            # mutant_scores_mapping: {[token, subtoken]: []}
            if header2_match:

                if header2_match.group(3).startswith("\"") or \
                        header2_match.group(3).startswith("\'"):

                    token_prob_position_score[int(header2_match.group(1)),
                                              header2_match.group(3)[1:-1]] = \
                        [float(header2_match.group(4)), float(
                            header2_match.group(5))]

                elif header2_match.group(3).startswith("true") or \
                        header2_match.group(3).startswith("false") or \
                        header2_match.group(3).startswith("0xf"):
                    token_prob_position_score[int(header2_match.group(1)),
                                              header2_match.group(3)] = \
                        [float(header2_match.group(4)), float(
                            header2_match.group(5))]
                else:

                    token_prob_position_score[int(header2_match.group(1)),
                                              header2_match.group(3)] = \
                        [float(header2_match.group(4)), float(
                            header2_match.group(5))]
            continue
            raise ValueError("No pattern matched line: {}".format(line))
    return token_prob_position_score


def combine_mapping(mutant_token_mapping, token_prob_position_score):
    """ Combines the mappings of mutant identifiers to their token
    identifiers and token identifiers to their naturalness scores to create a
    mutant identifier to naturalness score mapping

    :param mutant_token_mapping: Dict[int, int[]
            A dictionary containing a mapping from the mutant identifier to
            its token identifier
    :param token_prob_position_score: Dict[int, int[]
            A dictionary containing a mapping from the mutant identifier to
            its token identifier
    :return: Dict[int[], int[]]]
            A mapping of mutant identifiers to naturalness scores

    """
    # for all the mutants
    mutant_prob_position_score: Optional[
        Dict[int[-1, -1], int[-1, -1]]] = dict()

    for mutant in mutant_token_mapping:
        for tokenSub in token_prob_position_score:
            if mutant_token_mapping[mutant][0] == tokenSub[0]:

                # if mutant == 754:
                #     print("mutant token mapping:", mutant_token_mapping[mutant][
                #         1], "token bug:", tokenSub[1])

                if mutant_token_mapping[mutant][1] == tokenSub[1]:
                    mutant_prob_position_score[mutant] = \
                        token_prob_position_score[tokenSub]
                    # mutants_added.add(mutant)
                    # count += 1
                    # print("\tcount: ",count)
                    break
                if mutant_token_mapping[mutant][1] == "' '" and tokenSub[1] \
                        == " ":
                    mutant_prob_position_score[mutant] = \
                        token_prob_position_score[tokenSub]
                    break
                if mutant_token_mapping[mutant][1] == "0L" and tokenSub[1] \
                        == "0":
                    mutant_prob_position_score[mutant] = \
                        token_prob_position_score[tokenSub]
                    break
                if mutant_token_mapping[mutant][1].endswith(".0F") and \
                        mutant_token_mapping[mutant][1][:-3] == tokenSub[1]:
                    mutant_prob_position_score[mutant] = \
                        token_prob_position_score[tokenSub]
                    break
                # print(type(mutant))
                try:
                    if float(mutant_token_mapping[mutant][1]) == float(
                            tokenSub[1]):
                        mutant_prob_position_score[mutant] = \
                            token_prob_position_score[tokenSub]
                        break

                except:
                    continue

    return mutant_prob_position_score


def generate_eval_plot(sorted_mutants, killmap, rev_killmap,
                       total_number_of_mutants):
    """

    :param sorted_mutants: list [int]
            A pre-sorted list of mutant identifiers
    :param killmap: dict[frozenset: set()]
        A mapping from a set of identifiers from mutants killed to a
        set of identifiers for tests that kill each mutant.
    :param rev_killmap: dict[frozenset: set()]
        A mapping from a set of identifiers of tests to the
        set of identifiers for mutant that they kill.
    :param total_number_of_mutants: int
        The total number of mutants that were generated for this bug for this
        evaluation
    :return: list[float]
        A list of y-coordinates for the plot points representing work
        evaluation for a given type of mutants.
    """

    mutants_killed_so_far = set()
    plot: Optional[list[list]] = []
    plot.append(0)

    killmap_size = len(killmap)

    while True:
        # check if we are done whether it is by reaching the end or the plot
        # limit

        # Considers the total number of mutants
        if len(mutants_killed_so_far) == killmap_size:
            break

        # for the first mutant on the list of remaining mutants mutant select a test
        mutant_formatted_in_frozenset = sorted_mutants[0]

        # check whether the mutant was killed
        if mutant_formatted_in_frozenset in killmap.keys():
            # randomly select a test from the set off tests that kill that mutant
            randomly_selected_test = \
                random.sample(killmap[mutant_formatted_in_frozenset], 1)[0]
            # popping the key: test that kills all the value: mutant rev_killmap
            new_kills = rev_killmap.pop(randomly_selected_test)
            # popping all the mutants killed from killmap
            for mutant in new_kills:
                if mutant in killmap:
                    del killmap[mutant]
                    sorted_mutants.remove(mutant)

            # Remove the tests from all mutants.
            # this has to be done in a round about way
            for mutant in killmap.copy():
                # get all the remaining tests for that mutant minus the
                # currently randomly selected test
                randomly_selected_test_set = set()
                randomly_selected_test_set.add(randomly_selected_test)
                temp_mutant_test_set = killmap[mutant].difference(
                    randomly_selected_test_set)

                # if the test set is empty just break out of this inner loop
                if temp_mutant_test_set == set():
                    break
                # otherwise add the remaining tests back in
                else:
                    killmap[mutant] = temp_mutant_test_set

            mutants_killed_so_far = mutants_killed_so_far.union(new_kills)
            count = 0
            for mutant in mutants_killed_so_far:
                count += len(mutant)

            plot.append((count / total_number_of_mutants) * 100)
        else:

            sorted_mutants = sorted_mutants[1:]

    return plot, count


# TODO fix documentation
def plot(plot1, plot2, plot3, plot4, plot5):
    """Plots the test completeness graph


    Parameters:
         plot1: List[tuple(int, int)]
            A list of plot points that could used to plot test completeness
            """

    plotter = pd.DataFrame(
        data=plot1,
        columns=["Traditional Random"]
    )

    plotter2 = pd.DataFrame(
        data=plot2,
        columns=["Traditional Mutants (Best Case)"]
    )

    plotter3 = pd.DataFrame(
        data=plot3,
        columns=["All Natural"]
    )

    plotter4 = pd.DataFrame(
        data=plot4,
        columns=["Natural Mutants (Best Case)"]

    )
    plotter5 = pd.DataFrame(
        data=plot5,
        columns=["All Natural Mutants Random"]

    )
    maxi = max(len(plot1), len(plot2), len(plot3), len(plot4), len(plot5))
    increment = int(max(maxi, 1) / 10)

    ax = plotter.plot(fontsize=6,
                      xticks=(range(0, maxi, increment)),
                      yticks=range(0, 105, 25),
                      legend=False)

    plotter2.plot(fontsize=6,
                  xticks=(range(0, maxi, increment)),
                  yticks=(range(0, 105, 25)),
                  legend=False, ax=ax)

    plotter3.plot(fontsize=6,
                  xticks=(range(0, maxi, increment)),
                  yticks=(range(0, 105, 25)),
                  legend=False, ax=ax)

    plotter4.plot(fontsize=6,
                  xticks=(range(0, maxi, increment)),
                  yticks=(range(0, 105, 25)),
                  legend=False, ax=ax)

    plotter5.plot(fontsize=6,
                  xticks=(range(0, maxi, increment)),
                  yticks=(range(0, 105, 25)),
                  legend=False, ax=ax)

    plt.legend()
    ax.set_xlabel("Work")
    ax.set_ylabel("Test Completeness")

    return plt


def generate_test_completeness_plot(kill_map):
    """Generates the test completeness plot

    Takes a mapping of mutants to the tests that kill them.
    Calls calculate_dominating_mutants on the given kill_map to store a
    dominator set of mutants.

    get_tests_covered is called on each of the mutants in the dominator
    set to generate a mapping of each dominator mutant to the set of tests
    (mutant test set) that kill that mutant.

    For each point plotted (each iteration) the tests that are used in the
    plot (tests that kill the presented mutant) are subtracted from the
    remaining mutants' test set. That is, if test t_A kills mutants m1, m2,
    and m3, once we explore mutant m1 and count test t_A towards one plotted
    point, we can no longer count it for m2, and m3 in subsequent plotted
    points.

    After each iteration the remaining mutants are re-sorted based on the
    number of remaining tests that kill them.

    The above process is repeated until all dominating mutants are considered.

    Parameters:
        kill_map: A mapping from a set of identifiers from mutants killed to a
        set of identifiers for tests that kill each mutant.
    Returns:
        plot: List[tuple(int, int)]
            A list of plot points that could used to plot test completeness
    """

    # Get the dominator set of mutants and their graph
    result = calculate_dominating_mutants(kill_map)
    dominator_set = result[2]
    graph = result[0]
    plot = [0]

    dominator_to_subsumed_mapping: Optional[dict()] = dict()

    for dominator_mutant in dominator_set:
        temp = dominator_mutant.get_descendents()
        dominator_to_subsumed_mapping[dominator_mutant] = temp

    sorted_dominator_mutants = sorted(dominator_to_subsumed_mapping.keys(),
                                      key=lambda i:
                                      total_subsumed_size(i),
                                      reverse=True)

    # making sure each subsumed mutant is removed once using a triangular
    # double-loop
    mutants_considered_so_far: Optional[set()] = set()
    for dominator_mutant_index in range(0, len(sorted_dominator_mutants) - 1):
        for dominator_mutant_index2 in range(dominator_mutant_index + 1,
                                             len(sorted_dominator_mutants)):
            if dominator_to_subsumed_mapping[sorted_dominator_mutants[
                dominator_mutant_index2]] != set() and not \
                    sorted_dominator_mutants[
                        dominator_mutant_index2] in mutants_considered_so_far:
                dominator_to_subsumed_mapping[sorted_dominator_mutants[
                    dominator_mutant_index2]] = \
                    dominator_to_subsumed_mapping[sorted_dominator_mutants[
                        dominator_mutant_index2]].difference(
                        dominator_to_subsumed_mapping[sorted_dominator_mutants[
                            dominator_mutant_index]])

        # re-sort
        sorted_dominator_mutants = sorted(dominator_to_subsumed_mapping.keys(),
                                          key=lambda i:
                                          total_subsumed_size(i),
                                          reverse=True)
        mutants_considered_so_far.add(
            sorted_dominator_mutants[dominator_mutant_index])

    # we are now guaranteed to have unique subsumed mutants for each dominator
    # therefore, we can just plot the test completeness
    # Count the size of each node

    current_count = 0
    for dominator_mutant in sorted_dominator_mutants:
        current_count += dominator_mutant.size
        for mutant in dominator_to_subsumed_mapping[dominator_mutant]:
            current_count = current_count + mutant.size
        plot.append((current_count / len(kill_map)) * 100)

    return plot


def total_subsumed_size(mutant):
    result = mutant.size
    descendents = mutant.get_descendents()
    for descendent in descendents:
        result += descendent.size
    return result


if __name__ == "__main__":
    import sys

    if len(sys.argv) != 2:
        print("Usage <resultsPath>")
        sys.exit(-1)
    # TODO have to run the same analysis for each bug
    for i in range(1, 3):
        try:
            results_dir = sys.argv[1] + "Lang\\" + str(i) + "\killmatrix\\"
            natural_mutant_baseline_plots_counts = generate_all_natural_work_evaluation(
                results_dir)
            traditional_mutant_plots_counts = mutants_average(
                results_dir, "traditional-mutants", 10)
            all_natural = plot_generator(results_dir, "natural-mutants")
            all_trad = plot_generator(results_dir, "traditional-mutants")
            random_natural = mutants_average(
                results_dir, "natural-mutants", 10)
            graph = plot(traditional_mutant_plots_counts,
                         all_trad,
                         natural_mutant_baseline_plots_counts[0][0],
                         all_natural,
                         random_natural
                         )
            graph.savefig("images2\\" + "lang_" + str(i) + ".png", dpi=800)
        except:
            print("skipping bug number: ", i)
            continue
