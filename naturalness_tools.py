import csv
import re
from typing import Optional, Dict


def generate_mutant_to_token_mapping(log_file):
    """ Given mutants.log file from tailored mutants data for a bug,
    it creates a mapping from the mutant identifier to its token identifier


    :param log_file: str
                The path to mutants.log generated by tailored mutant data
    :return: Dict[int, int[]
            A dictionary containing a mapping from the mutant identifier to
            its token identifier
    """
    mutant_to_token_mapping: Optional[Dict[int, int[-1, -1]]] = dict()
    tokenization_pattern = re.compile(
        r"([0-9]*);[A-Z]*;([0-9]*);((([0-9]*[.]*)[ ]*[a-z]*[A-Z]*[.]*[:]*[-]*[,]*[_]*[\"]*[ ]*[']*[()]*[\[]*[\]]*)*)")
    with open(log_file, 'r') as fo:
        for line in fo:
            header_match = tokenization_pattern.match(line)
            if header_match:
                if header_match.group(3).startswith("\'") or header_match.group(
                        3).startswith("\""):
                    subtoken = header_match.group(3)[1:-1]
                else:
                    subtoken = header_match.group(3)
                mutant_to_token_mapping[int(header_match.group(1))] = [
                    int(header_match.group(2)), subtoken]
            continue
            raise ValueError("No pattern matched line: {}".format(line))

        return mutant_to_token_mapping


def generate_scores(mml_csv):
    """ Uses mml_confidence_data.csv file from tailored mutants
    data for a given bug. It creates a mapping from the token identifiers for
    that bug to their naturalness scores.

    :param log_file: str
                The path to mutants.log generated by tailored mutant data
    :return: Dict[int, int[]
            A dictionary containing a mapping from the mutant identifier to
            its token identifier
    """
    token_prob_position_score: Optional[Dict[int[-1, -1], int[-1, -1]]] = dict()
    score_reading_pattern = re.compile(
        r"[A-Z_(]*([\d]*)[)]<([\d]*[\"]*[']*[ ]*[a-z]*[A-Z]*[.]*[:]*[\_]*[(]*[)]*[>]*)*-> ([\"'\d*[a-z]]*[.a-zA-Z0-9_\"\/',<> $?\[\]:()\\-]*)#(\d.\d*[A-Z]*-*[0-9]*)*#(\d.\d*[A-Z]*-*[0-9]*)*#[a-z]*#(\d.\d*)([A-Z]-[0-9])*#[a-z]*#[a-z]*#(\d.\d*)([A-Z]-[0-9])*")

    counter = 0
    with open(mml_csv, 'r') as fo:
        for line in fo:
            # counter = counter + 1
            # if counter > 66582:
            #     print(counter)
            if line.strip() == "\n" or line == "\n":
                continue
            header2_match = score_reading_pattern.match(line)
            # group 1 -> token number identifier
            # group 3 -> subtoken number identifier
            # group 5 -> first
            # group 9 -> second
            # mutant_scores_mapping: {[token, subtoken]: []}
            if header2_match:

                if header2_match.group(3).startswith("\"") or \
                        header2_match.group(3).startswith("\'"):

                    token_prob_position_score[int(header2_match.group(1)),
                                              header2_match.group(3)[1:-1]] = \
                        [float(header2_match.group(4)), float(
                            header2_match.group(5))]

                elif header2_match.group(3).startswith("true") or \
                        header2_match.group(3).startswith("false") or \
                        header2_match.group(3).startswith("0xf"):
                    token_prob_position_score[int(header2_match.group(1)),
                                              header2_match.group(3)] = \
                        [float(header2_match.group(4)), float(
                            header2_match.group(5))]
                else:

                    token_prob_position_score[int(header2_match.group(1)),
                                              header2_match.group(3)] = \
                        [float(header2_match.group(4)), float(
                            header2_match.group(5))]
            continue
            raise ValueError("No pattern matched line: {}".format(line))
        # print("out")
    return token_prob_position_score


def combine_mapping(mutant_token_mapping, token_prob_position_score):
    """ Combines the mappings of mutant identifiers to their token
    identifiers and token identifiers to their naturalness scores to create a
    mutant identifier to naturalness score mapping

    :param mutant_token_mapping: Dict[int, int[]
            A dictionary containing a mapping from the mutant identifier to
            its token identifier
    :param token_prob_position_score: Dict[int, int[]
            A dictionary containing a mapping from the mutant identifier to
            its token identifier
    :return: Dict[int[], int[]]]
            A mapping of mutant identifiers to naturalness scores

    """
    # for all the mutants
    mutant_prob_position_score: Optional[
        Dict[int[-1, -1], int[-1, -1]]] = dict()
    counter = 0
    for mutant in mutant_token_mapping:
        for tokenSub in token_prob_position_score:
            if mutant_token_mapping[mutant][0] == tokenSub[0]:

                # if mutant == 754:
                #     print("mutant token mapping:", mutant_token_mapping[mutant][
                #         1], "token bug:", tokenSub[1])

                if mutant_token_mapping[mutant][1] == tokenSub[1]:
                    mutant_prob_position_score[mutant] = \
                        token_prob_position_score[tokenSub]
                    # mutants_added.add(mutant)
                    # count += 1
                    # print("\tcount: ",count)
                    break
                if mutant_token_mapping[mutant][1] == "' '" and tokenSub[1] \
                        == " ":
                    mutant_prob_position_score[mutant] = \
                        token_prob_position_score[tokenSub]
                    break
                if mutant_token_mapping[mutant][1] == "0L" and tokenSub[1] \
                        == "0":
                    mutant_prob_position_score[mutant] = \
                        token_prob_position_score[tokenSub]
                    break
                if mutant_token_mapping[mutant][1].endswith(".0F") and \
                        mutant_token_mapping[mutant][1][:-3] == tokenSub[1]:
                    mutant_prob_position_score[mutant] = \
                        token_prob_position_score[tokenSub]
                    break
                # print(type(mutant))
                try:
                    if float(mutant_token_mapping[mutant][1]) == float(
                            tokenSub[1]):
                        mutant_prob_position_score[mutant] = \
                            token_prob_position_score[tokenSub]
                        break

                except:
                    continue

    return mutant_prob_position_score


# TODO document
def natural_offset_killmap(results_dir):
    # create a temp offset killmap

    csv_filename2 = results_dir + \
                    "\\traditional-mutants\\non-triggering\KillMap.csv"
    offset = 0
    with open(csv_filename2, newline='') as File0:
        with open("temp_killMap.csv", "w", newline='') as File3:
            reader = csv.reader(File0)
            for line in File0:
                ++offset
                File3.write(line)
            offset += 20000

    csv_filename = results_dir + \
                   "\\natural-mutants\\non-triggering\KillMap.csv"

    with open(csv_filename, newline='') as File:
        with open("temp_killMap.csv", "a", newline='') as File2:
            kill_map = {}
            reader = csv.reader(File)
            readerSize = csv.reader(File)
            writer = csv.writer(File2)

            # skipping the header
            next(reader)
            File2.write(str(File.readline()))
            # print(readerSize)
            empty_csv_check = next(readerSize, "empty")
            if empty_csv_check != "empty" and len(empty_csv_check) == 2:

                for k, y in reader:
                    # converting to integers
                    k = int(k)
                    y = int(y)
                    output_row = (k, y + offset + 100)
                    writer.writerow(output_row)
